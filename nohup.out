[I 14:48:09.087 NotebookApp] Serving notebooks from local directory: /root/spark-docker-swarm
[I 14:48:09.087 NotebookApp] The Jupyter Notebook is running at:
[I 14:48:09.087 NotebookApp] http://(isislab21 or 127.0.0.1):9999/
[I 14:48:09.087 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 14:48:38.030 NotebookApp] 302 GET / (10.66.185.203) 1.06ms
[I 14:48:38.043 NotebookApp] 302 GET /tree? (10.66.185.203) 0.93ms
[I 14:48:46.610 NotebookApp] 302 POST /login?next=%2Ftree%3F (10.66.185.203) 1.92ms
[I 14:49:19.073 NotebookApp] Creating new notebook in 
[I 14:49:19.080 NotebookApp] Writing notebook-signing key to /root/.local/share/jupyter/notebook_secret
[I 14:49:21.413 NotebookApp] Kernel started: beb22788-d1ff-4735-a952-30d2d3cdfdd2
[I 14:49:21.919 NotebookApp] Adapting to protocol v5.1 for kernel beb22788-d1ff-4735-a952-30d2d3cdfdd2
2018-12-11 14:50:46 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2018-12-11 14:50:47 ERROR SparkContext:91 - Error initializing SparkContext.
java.lang.Exception: Yarn Local dirs can't be empty
	at org.apache.spark.util.Utils$.getYarnLocalDirs(Utils.scala:902)
	at org.apache.spark.util.Utils$.getConfiguredLocalDirs(Utils.scala:843)
	at org.apache.spark.storage.DiskBlockManager.createLocalDirs(DiskBlockManager.scala:139)
	at org.apache.spark.storage.DiskBlockManager.<init>(DiskBlockManager.scala:42)
	at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:143)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:349)
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:175)
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:424)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
[I 14:51:21.395 NotebookApp] Saving file at /sparkTest.ipynb
[I 15:33:21.406 NotebookApp] Saving file at /sparkTest.ipynb
2018-12-11 15:33:54 WARN  SparkContext:66 - Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.GatewayConnection.run(GatewayConnection.java:238)
java.lang.Thread.run(Thread.java:748)
2018-12-11 15:33:54 ERROR SparkContext:91 - Error initializing SparkContext.
java.lang.Exception: Yarn Local dirs can't be empty
	at org.apache.spark.util.Utils$.getYarnLocalDirs(Utils.scala:902)
	at org.apache.spark.util.Utils$.getConfiguredLocalDirs(Utils.scala:843)
	at org.apache.spark.storage.DiskBlockManager.createLocalDirs(DiskBlockManager.scala:139)
	at org.apache.spark.storage.DiskBlockManager.<init>(DiskBlockManager.scala:42)
	at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:143)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:349)
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:175)
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:424)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
[I 15:35:21.703 NotebookApp] Saving file at /sparkTest.ipynb
[I 16:04:45.063 NotebookApp] Starting buffering for beb22788-d1ff-4735-a952-30d2d3cdfdd2:710320581a084d47998943647233955c
[I 16:14:04.476 NotebookApp] Adapting to protocol v5.1 for kernel beb22788-d1ff-4735-a952-30d2d3cdfdd2
[I 16:54:33.988 NotebookApp] Starting buffering for beb22788-d1ff-4735-a952-30d2d3cdfdd2:28c96c4b03b2460083f1160cb72ec6d8
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
